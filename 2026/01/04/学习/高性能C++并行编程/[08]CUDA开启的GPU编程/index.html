<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/favicon.png"/>
	<link rel="shortcut icon" href="/img/favicon.png">
	
			    <title>
    楚天
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="Chutian Duan blog 深度学习 C++" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">Enter Blog</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="category-link" href="/categories/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">动手深度学习</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/Linux%E9%AB%98%E6%80%A7%E8%83%BD%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%96%E7%A8%8B/">Linux高性能服务器编程</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E7%A0%81%E7%BB%83%E4%B9%A0/">代码练习</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E6%97%B6%E7%AB%9E%E6%8A%80%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/">实时竞技游戏开发</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/%E7%8E%B0%E4%BB%A3C-%E5%AE%9E%E8%B7%B5/">现代C++实践</a></li><li><a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%80%A7%E8%83%BDC-%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B/">高性能C++并行编程</a></li><li><a class="category-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li><a class="category-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/%E8%AF%BB%E4%B9%A6%E6%9C%89%E6%84%9F/">读书有感</a></li><li><a class="category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li><a class="category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%89%A9%E6%95%A3/">扩散</a></li><li><a class="category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%B6%85%E5%88%86/">超分</a></li><li><a class="category-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li><a class="category-link" href="/categories/%E7%94%9F%E6%B4%BB/%E6%97%85%E6%B8%B8%E6%94%BB%E7%95%A5/">旅游攻略</a></li><li><a class="category-link" href="/categories/%E8%BD%AF%E4%BB%B6/">软件</a>
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        <li class="active">
	            <a href="#s1">归档</a>
	                    <ul class="submenu">
	                        <li>
	                        <a class="archive-link" href="/archives/2026/02/">二月 2026</a></li><li><a class="archive-link" href="/archives/2026/01/">一月 2026</a></li><li><a class="archive-link" href="/archives/2025/12/">十二月 2025</a></li><li><a class="archive-link" href="/archives/2025/11/">十一月 2025</a></li><li><a class="archive-link" href="/archives/2024/09/">九月 2024</a></li><li><a class="archive-link" href="/archives/2024/08/">八月 2024</a>
	                    </ul>
	        </li>
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="关于">
		                关于
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tags/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/ChutianDuan" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(/img/covers/cover2.jpg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >CUDA开启的GPU编程</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="CUDA-开启的-GPU-编程"><a href="#CUDA-开启的-GPU-编程" class="headerlink" title="CUDA 开启的 GPU 编程"></a>CUDA 开启的 GPU 编程</h1><p>time：2025_12_26</p>
<hr>
<h2 id="1-工程与编译（CMake-nvcc）"><a href="#1-工程与编译（CMake-nvcc）" class="headerlink" title="1. 工程与编译（CMake &#x2F; nvcc）"></a>1. 工程与编译（CMake &#x2F; nvcc）</h2><h3 id="1-1-最小-CMake-工程"><a href="#1-1-最小-CMake-工程" class="headerlink" title="1.1 最小 CMake 工程"></a>1.1 最小 CMake 工程</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.18</span>)</span><br><span class="line"><span class="keyword">project</span>(hellocuda LANGUAGES CXX CUDA)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(main main.cu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set_target_properties</span>(main PROPERTIES</span><br><span class="line">  CUDA_STANDARD <span class="number">17</span></span><br><span class="line">  CUDA_STANDARD_REQUIRED <span class="keyword">ON</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要跨 .cu 调用 device 函数/做 device link 时开启</span></span><br><span class="line"><span class="keyword">set_target_properties</span>(main PROPERTIES CUDA_SEPARABLE_COMPILATION <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用 CUDA 编译选项（按需启用）</span></span><br><span class="line"><span class="keyword">target_compile_options</span>(main PUBLIC</span><br><span class="line">  $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--expt-relaxed-constexpr&gt;</span><br><span class="line">  $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--expt-extended-lambda&gt;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="1-2-推荐的基础错误检查宏"><a href="#1-2-推荐的基础错误检查宏" class="headerlink" title="1.2 推荐的基础错误检查宏"></a>1.2 推荐的基础错误检查宏</h3><p>避免依赖 sample 的 <code>helper_cuda.h</code>，直接自带一个最小版：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CUDA_CHECK(call) do &#123;                                  \</span></span><br><span class="line"><span class="meta">  cudaError_t err = (call);                                    \</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span> (err != cudaSuccess) &#123;                                    \</span></span><br><span class="line"><span class="meta">    std::fprintf(stderr, <span class="string">&quot;CUDA error %s:%d: %s\n&quot;</span>,             \</span></span><br><span class="line"><span class="meta">      __FILE__, __LINE__, cudaGetErrorString(err));            \</span></span><br><span class="line"><span class="meta">    std::exit(1);                                              \</span></span><br><span class="line"><span class="meta">  &#125;                                                            \</span></span><br><span class="line"><span class="meta">&#125; while(0)</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-CUDA-基础：函数修饰符与执行位置"><a href="#2-CUDA-基础：函数修饰符与执行位置" class="headerlink" title="2. CUDA 基础：函数修饰符与执行位置"></a>2. CUDA 基础：函数修饰符与执行位置</h2><h3 id="2-1-global-device-host"><a href="#2-1-global-device-host" class="headerlink" title="2.1 __global__ / __device__ / __host__"></a>2.1 <code>__global__ / __device__ / __host__</code></h3><ul>
<li><p><code>__global__</code>：核函数（kernel）</p>
<ul>
<li>在 GPU 上并行执行</li>
<li>由主机端（CPU）发起 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code></li>
<li>返回类型必须为 <code>void</code>（通常通过指针写回结果）</li>
</ul>
</li>
<li><p><code>__device__</code>：设备函数</p>
<ul>
<li>在 GPU 上执行</li>
<li>只能从 device&#x2F;global 调用</li>
</ul>
</li>
<li><p><code>__host__</code>：主机函数</p>
<ul>
<li>在 CPU 上执行</li>
<li>未标注的普通函数默认就是 host</li>
</ul>
</li>
</ul>
<p>组合：</p>
<ul>
<li><code>__host__ __device__</code>：同一函数在 CPU&#x2F;GPU 两侧都可用（注意 device 侧不支持完整的 C++ 标准库能力）</li>
</ul>
<h3 id="2-2-CUDA-ARCH-（区分-device-host-编译路径）"><a href="#2-2-CUDA-ARCH-（区分-device-host-编译路径）" class="headerlink" title="2.2 __CUDA_ARCH__（区分 device&#x2F;host 编译路径）"></a>2.2 <code>__CUDA_ARCH__</code>（区分 device&#x2F;host 编译路径）</h3><ul>
<li><code>__CUDA_ARCH__</code> 只在 device 编译路径中定义，值为计算能力架构号（例如 750&#x2F;800 等）</li>
<li>常用于同一个函数在 host&#x2F;device 的条件编译</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__host__ __device__ inline int where_am_i() &#123;</span><br><span class="line">#ifdef __CUDA_ARCH__</span><br><span class="line">    return __CUDA_ARCH__;   // device：架构号</span><br><span class="line">#else</span><br><span class="line">    return -1;              // host：标记</span><br><span class="line">#endif</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-3-constexpr-与-device-代码"><a href="#2-3-constexpr-与-device-代码" class="headerlink" title="2.3 constexpr 与 device 代码"></a>2.3 <code>constexpr</code> 与 device 代码</h3><ul>
<li>如需更宽松的 <code>constexpr</code> 在 device 侧工作，常用 <code>--expt-relaxed-constexpr</code></li>
<li>device 侧 lambda 扩展常用 <code>--expt-extended-lambda</code></li>
</ul>
<hr>
<h2 id="3-Kernel-启动、线程块模型与索引"><a href="#3-Kernel-启动、线程块模型与索引" class="headerlink" title="3. Kernel 启动、线程块模型与索引"></a>3. Kernel 启动、线程块模型与索引</h2><h3 id="3-1-Kernel-启动语法"><a href="#3-1-Kernel-启动语法" class="headerlink" title="3.1 Kernel 启动语法"></a>3.1 Kernel 启动语法</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;grid, block, shared_bytes, stream&gt;&gt;&gt;(args...);</span><br></pre></td></tr></table></figure>

<ul>
<li>常用：<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code></li>
<li><code>shared_bytes</code>：动态共享内存字节数（默认 0）</li>
<li><code>stream</code>：CUDA 流（默认 0）</li>
</ul>
<h3 id="3-2-线程-块索引"><a href="#3-2-线程-块索引" class="headerlink" title="3.2 线程&#x2F;块索引"></a>3.2 线程&#x2F;块索引</h3><ul>
<li><code>threadIdx.&#123;x,y,z&#125;</code>：线程在块内索引</li>
<li><code>blockIdx.&#123;x,y,z&#125;</code>：块在网格内索引</li>
<li><code>blockDim.&#123;x,y,z&#125;</code>：每块线程数维度</li>
<li><code>gridDim.&#123;x,y,z&#125;</code>：网格块数维度</li>
</ul>
<h3 id="3-3-典型打印示例（便于理解执行模型）"><a href="#3-3-典型打印示例（便于理解执行模型）" class="headerlink" title="3.3 典型打印示例（便于理解执行模型）"></a>3.3 典型打印示例（便于理解执行模型）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cstdio&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void kernel() &#123;</span><br><span class="line">    printf(&quot;block %d/%d, thread %d/%d\n&quot;,</span><br><span class="line">           blockIdx.x, gridDim.x,</span><br><span class="line">           threadIdx.x, blockDim.x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    kernel&lt;&lt;&lt;2, 3&gt;&gt;&gt;();</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-4-Grid-Stride-Loop（通用遍历范式）"><a href="#3-4-Grid-Stride-Loop（通用遍历范式）" class="headerlink" title="3.4 Grid-Stride Loop（通用遍历范式）"></a>3.4 Grid-Stride Loop（通用遍历范式）</h3><p>适用于任意大小数据与任意 grid&#x2F;block 配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">__global__ void work(int* a, int n) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n;</span><br><span class="line">         i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        a[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-同步与错误处理"><a href="#4-同步与错误处理" class="headerlink" title="4. 同步与错误处理"></a>4. 同步与错误处理</h2><h3 id="4-1-CPU-GPU-默认异步"><a href="#4-1-CPU-GPU-默认异步" class="headerlink" title="4.1 CPU&#x2F;GPU 默认异步"></a>4.1 CPU&#x2F;GPU 默认异步</h3><ul>
<li><p>kernel launch 对 host 来说通常是异步的</p>
</li>
<li><p>常用同步：</p>
<ul>
<li><code>cudaDeviceSynchronize()</code>：等待当前设备上所有已提交工作完成</li>
<li><code>cudaStreamSynchronize(stream)</code>：等待某个流完成</li>
</ul>
</li>
</ul>
<h3 id="4-2-推荐的-launch-后检查模板"><a href="#4-2-推荐的-launch-后检查模板" class="headerlink" title="4.2 推荐的 launch 后检查模板"></a>4.2 推荐的 launch 后检查模板</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(...);</span><br><span class="line"><span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line"><span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="5-内存管理（Host-Device-Unified）"><a href="#5-内存管理（Host-Device-Unified）" class="headerlink" title="5. 内存管理（Host&#x2F;Device&#x2F;Unified）"></a>5. 内存管理（Host&#x2F;Device&#x2F;Unified）</h2><h3 id="5-1-经典模式：cudaMalloc-cudaMemcpy"><a href="#5-1-经典模式：cudaMalloc-cudaMemcpy" class="headerlink" title="5.1 经典模式：cudaMalloc + cudaMemcpy"></a>5.1 经典模式：<code>cudaMalloc + cudaMemcpy</code></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cstdio&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">#define CUDA_CHECK(call) do &#123; \</span><br><span class="line">  cudaError_t err = (call); \</span><br><span class="line">  if (err != cudaSuccess) &#123; \</span><br><span class="line">    std::fprintf(stderr, &quot;CUDA error: %s\n&quot;, cudaGetErrorString(err)); \</span><br><span class="line">    std::exit(1); \</span><br><span class="line">  &#125; \</span><br><span class="line">&#125; while(0)</span><br><span class="line"></span><br><span class="line">__global__ void kernel(int* out) &#123; *out = 42; &#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    int* d_out = nullptr;</span><br><span class="line">    CUDA_CHECK(cudaMalloc(&amp;d_out, sizeof(int)));</span><br><span class="line"></span><br><span class="line">    kernel&lt;&lt;&lt;1,1&gt;&gt;&gt;(d_out);</span><br><span class="line">    CUDA_CHECK(cudaGetLastError());</span><br><span class="line">    CUDA_CHECK(cudaDeviceSynchronize());</span><br><span class="line"></span><br><span class="line">    int h_out = 0;</span><br><span class="line">    CUDA_CHECK(cudaMemcpy(&amp;h_out, d_out, sizeof(int), cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    std::printf(&quot;ret=%d\n&quot;, h_out);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaFree(d_out));</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-统一内存（Unified-Memory）：cudaMallocManaged"><a href="#5-2-统一内存（Unified-Memory）：cudaMallocManaged" class="headerlink" title="5.2 统一内存（Unified Memory）：cudaMallocManaged"></a>5.2 统一内存（Unified Memory）：<code>cudaMallocManaged</code></h3><ul>
<li>一份指针同时可被 CPU&#x2F;GPU 访问</li>
<li>常配合同步；性能敏感时可用预取提升稳定性</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cstdio&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void fill(int* a, int n) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        a[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    int n = 32;</span><br><span class="line">    int* a = nullptr;</span><br><span class="line">    cudaMallocManaged(&amp;a, sizeof(int) * n);</span><br><span class="line"></span><br><span class="line">    fill&lt;&lt;&lt;1, 128&gt;&gt;&gt;(a, n);</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i &lt; n; ++i) std::printf(&quot;a[%d]=%d\n&quot;, i, a[i]);</span><br><span class="line">    cudaFree(a);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-3-预取（Prefetch）与驻留优化（进阶但常用）"><a href="#5-3-预取（Prefetch）与驻留优化（进阶但常用）" class="headerlink" title="5.3 预取（Prefetch）与驻留优化（进阶但常用）"></a>5.3 预取（Prefetch）与驻留优化（进阶但常用）</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaGetDevice</span>(&amp;dev);</span><br><span class="line"><span class="built_in">cudaMemPrefetchAsync</span>(a, <span class="built_in">sizeof</span>(<span class="type">int</span>)*n, dev, <span class="number">0</span>);     <span class="comment">// 预取到 GPU</span></span><br><span class="line"><span class="comment">// kernel ...</span></span><br><span class="line"><span class="built_in">cudaMemPrefetchAsync</span>(a, <span class="built_in">sizeof</span>(<span class="type">int</span>)*n, cudaCpuDeviceId, <span class="number">0</span>); <span class="comment">// 预取回 CPU</span></span><br><span class="line"><span class="built_in">cudaDeviceSynchronize</span>();</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6-C-封装：RAII-与可复用接口"><a href="#6-C-封装：RAII-与可复用接口" class="headerlink" title="6. C++ 封装：RAII 与可复用接口"></a>6. C++ 封装：RAII 与可复用接口</h2><h3 id="6-1-RAII-管理-Unified-Memory-指针（简单、可靠、适合入门）"><a href="#6-1-RAII-管理-Unified-Memory-指针（简单、可靠、适合入门）" class="headerlink" title="6.1 RAII 管理 Unified Memory 指针（简单、可靠、适合入门）"></a>6.1 RAII 管理 Unified Memory 指针（简单、可靠、适合入门）</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstddef&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ManagedArray</span> &#123;</span><br><span class="line">    T* p&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    <span class="type">size_t</span> n&#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">ManagedArray</span><span class="params">(<span class="type">size_t</span> n_)</span> : n(n_) &#123;</span></span><br><span class="line">        <span class="built_in">cudaMallocManaged</span>(&amp;p, <span class="built_in">sizeof</span>(T) * n);</span><br><span class="line">    &#125;</span><br><span class="line">    ~<span class="built_in">ManagedArray</span>() &#123;</span><br><span class="line">        <span class="keyword">if</span> (p) <span class="built_in">cudaFree</span>(p);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">T* <span class="title">data</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> p; &#125;</span><br><span class="line">    <span class="function"><span class="type">const</span> T* <span class="title">data</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> p; &#125;</span><br><span class="line">    T&amp; <span class="keyword">operator</span>[](<span class="type">size_t</span> i) &#123; <span class="keyword">return</span> p[i]; &#125;</span><br><span class="line">    <span class="type">const</span> T&amp; <span class="keyword">operator</span>[](<span class="type">size_t</span> i) <span class="type">const</span> &#123; <span class="keyword">return</span> p[i]; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h3 id="6-2-结合-kernel-使用"><a href="#6-2-结合-kernel-使用" class="headerlink" title="6.2 结合 kernel 使用"></a>6.2 结合 kernel 使用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__global__ void init(int* a, int n) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        a[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    int n = 1000;</span><br><span class="line">    ManagedArray&lt;int&gt; a(n);</span><br><span class="line">    init&lt;&lt;&lt;32, 128&gt;&gt;&gt;(a.data(), n);</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    // CPU 侧直接读</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>allocator 方式也可把 unified memory 接入 <code>std::vector</code>，但 allocator 细节较多；建议先把 RAII 指针与 <code>.data()</code> 传参掌握牢。</p>
</blockquote>
<hr>
<h2 id="7-Thrust-库：容器与算法（高层-CUDA）"><a href="#7-Thrust-库：容器与算法（高层-CUDA）" class="headerlink" title="7. Thrust 库：容器与算法（高层 CUDA）"></a>7. Thrust 库：容器与算法（高层 CUDA）</h2><h3 id="7-1-常用容器"><a href="#7-1-常用容器" class="headerlink" title="7.1 常用容器"></a>7.1 常用容器</h3><ul>
<li><code>thrust::host_vector&lt;T&gt;</code>：主机端 vector</li>
<li><code>thrust::device_vector&lt;T&gt;</code>：设备端 vector</li>
<li>通过赋值可触发 H2D &#x2F; D2H 拷贝（更准确地说：构造&#x2F;赋值会在 host&#x2F;device 容器之间进行数据迁移）</li>
</ul>
<h3 id="7-2-AXPY-示例（device-vector-自写-kernel）"><a href="#7-2-AXPY-示例（device-vector-自写-kernel）" class="headerlink" title="7.2 AXPY 示例（device_vector + 自写 kernel）"></a>7.2 AXPY 示例（device_vector + 自写 kernel）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;thrust/host_vector.h&gt;</span><br><span class="line">#include &lt;thrust/device_vector.h&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line">#include &lt;cstdio&gt;</span><br><span class="line"></span><br><span class="line">__global__ void axpy(float* x, const float* y, float a, int n) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        x[i] = a * x[i] + y[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    int n = 1 &lt;&lt; 20;</span><br><span class="line">    float a = 3.14f;</span><br><span class="line"></span><br><span class="line">    thrust::host_vector&lt;float&gt; hx(n), hy(n);</span><br><span class="line">    for (int i = 0; i &lt; n; ++i) &#123; hx[i] = i * 0.001f; hy[i] = 1.0f; &#125;</span><br><span class="line"></span><br><span class="line">    thrust::device_vector&lt;float&gt; dx = hx;</span><br><span class="line">    thrust::device_vector&lt;float&gt; dy = hy;</span><br><span class="line"></span><br><span class="line">    axpy&lt;&lt;&lt;256, 256&gt;&gt;&gt;(thrust::raw_pointer_cast(dx.data()),</span><br><span class="line">                       thrust::raw_pointer_cast(dy.data()),</span><br><span class="line">                       a, n);</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">    hx = dx;</span><br><span class="line">    std::printf(&quot;hx[0]=%f, hx[n-1]=%f\n&quot;, hx[0], hx[n-1]);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="8-原子操作（Atomic）"><a href="#8-原子操作（Atomic）" class="headerlink" title="8. 原子操作（Atomic）"></a>8. 原子操作（Atomic）</h2><h3 id="8-1-常用原子"><a href="#8-1-常用原子" class="headerlink" title="8.1 常用原子"></a>8.1 常用原子</h3><ul>
<li><code>atomicAdd / atomicSub</code></li>
<li><code>atomicAnd / atomicOr / atomicXor</code></li>
<li><code>atomicMin / atomicMax</code></li>
<li><code>atomicCAS</code>：Compare-And-Swap，可用于构造自定义原子操作</li>
</ul>
<h3 id="8-2-用-CAS-实现自定义原子加"><a href="#8-2-用-CAS-实现自定义原子加" class="headerlink" title="8.2 用 CAS 实现自定义原子加"></a>8.2 用 CAS 实现自定义原子加</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__device__ __forceinline__ int my_atomic_add(int* dst, int val) &#123;</span><br><span class="line">    int old = *dst;</span><br><span class="line">    int assumed;</span><br><span class="line">    do &#123;</span><br><span class="line">        assumed = old;</span><br><span class="line">        old = atomicCAS(dst, assumed, assumed + val);</span><br><span class="line">    &#125; while (assumed != old);</span><br><span class="line">    return old;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-3-朴素并行求和（全局原子累加）"><a href="#8-3-朴素并行求和（全局原子累加）" class="headerlink" title="8.3 朴素并行求和（全局原子累加）"></a>8.3 朴素并行求和（全局原子累加）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ void parallel_sum(int* sum, const int* arr, int n) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        atomicAdd(sum, arr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="9-线程块与共享内存（Shared-Memory）"><a href="#9-线程块与共享内存（Shared-Memory）" class="headerlink" title="9. 线程块与共享内存（Shared Memory）"></a>9. 线程块与共享内存（Shared Memory）</h2><h3 id="9-1-核心概念"><a href="#9-1-核心概念" class="headerlink" title="9.1 核心概念"></a>9.1 核心概念</h3><ul>
<li><code>__shared__</code>：块内共享内存（一个 block 内所有线程可见）</li>
<li><code>__syncthreads()</code>：块内同步屏障（必须保证同一 block 的线程都能到达）</li>
</ul>
<p>共享内存常用于：</p>
<ul>
<li>块内复用数据（减少 global memory 访问）</li>
<li>块内归约（reduce）</li>
<li>tile-based 计算（矩阵乘、卷积、图像算子）</li>
</ul>
<h3 id="9-2-块内归约：每块只做一次全局原子"><a href="#9-2-块内归约：每块只做一次全局原子" class="headerlink" title="9.2 块内归约：每块只做一次全局原子"></a>9.2 块内归约：每块只做一次全局原子</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void reduce_sum(const int* arr, int n, int* out) &#123;</span><br><span class="line">    extern __shared__ int sdata[]; // 动态共享内存</span><br><span class="line">    int tid = threadIdx.x;</span><br><span class="line">    int i = blockIdx.x * blockDim.x + tid;</span><br><span class="line"></span><br><span class="line">    sdata[tid] = (i &lt; n) ? arr[i] : 0;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    for (int s = blockDim.x / 2; s &gt; 0; s &gt;&gt;= 1) &#123;</span><br><span class="line">        if (tid &lt; s) sdata[tid] += sdata[tid + s];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (tid == 0) atomicAdd(out, sdata[0]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动方式（动态共享内存大小）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> threads = <span class="number">256</span>;</span><br><span class="line"><span class="type">int</span> blocks = (n + threads - <span class="number">1</span>) / threads;</span><br><span class="line">reduce_sum&lt;&lt;&lt;blocks, threads, <span class="function">threads * <span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(arr, n, out)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="9-3-Tile-示例：2D-图像-3x3-均值滤波（共享内存加速范式）"><a href="#9-3-Tile-示例：2D-图像-3x3-均值滤波（共享内存加速范式）" class="headerlink" title="9.3 Tile 示例：2D 图像 3x3 均值滤波（共享内存加速范式）"></a>9.3 Tile 示例：2D 图像 3x3 均值滤波（共享内存加速范式）</h3><p>适用于图像&#x2F;矩阵类任务（tile + halo）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void mean3x3(const float* in, float* out, int H, int W) &#123;</span><br><span class="line">    // blockDim = (Bx, By)</span><br><span class="line">    const int x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    const int y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    // tile 尺寸：块大小 + halo(上下左右各1)</span><br><span class="line">    const int Bx = blockDim.x;</span><br><span class="line">    const int By = blockDim.y;</span><br><span class="line">    extern __shared__ float tile[];</span><br><span class="line"></span><br><span class="line">    // tile 索引函数</span><br><span class="line">    auto t = [&amp;](int ty, int tx) -&gt; float&amp; &#123;</span><br><span class="line">        return tile[ty * (Bx + 2) + tx];</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    // 对应 tile 坐标（+1 是为了留 halo）</span><br><span class="line">    const int tx = threadIdx.x + 1;</span><br><span class="line">    const int ty = threadIdx.y + 1;</span><br><span class="line"></span><br><span class="line">    // 读主区域</span><br><span class="line">    float v = 0.f;</span><br><span class="line">    if (x &lt; W &amp;&amp; y &lt; H) v = in[y * W + x];</span><br><span class="line">    t(ty, tx) = v;</span><br><span class="line"></span><br><span class="line">    // 读 halo（边界处做 clamp 或置零，这里用置零策略）</span><br><span class="line">    if (threadIdx.x == 0) &#123;</span><br><span class="line">        float lv = (x &gt; 0 &amp;&amp; y &lt; H) ? in[y * W + (x - 1)] : 0.f;</span><br><span class="line">        t(ty, 0) = lv;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.x == Bx - 1) &#123;</span><br><span class="line">        float rv = (x + 1 &lt; W &amp;&amp; y &lt; H) ? in[y * W + (x + 1)] : 0.f;</span><br><span class="line">        t(ty, Bx + 1) = rv;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.y == 0) &#123;</span><br><span class="line">        float uv = (y &gt; 0 &amp;&amp; x &lt; W) ? in[(y - 1) * W + x] : 0.f;</span><br><span class="line">        t(0, tx) = uv;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.y == By - 1) &#123;</span><br><span class="line">        float dv = (y + 1 &lt; H &amp;&amp; x &lt; W) ? in[(y + 1) * W + x] : 0.f;</span><br><span class="line">        t(By + 1, tx) = dv;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 角落 halo（四个角）</span><br><span class="line">    if (threadIdx.x == 0 &amp;&amp; threadIdx.y == 0) &#123;</span><br><span class="line">        t(0,0) = (x&gt;0 &amp;&amp; y&gt;0) ? in[(y-1)*W + (x-1)] : 0.f;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.x == Bx-1 &amp;&amp; threadIdx.y == 0) &#123;</span><br><span class="line">        t(0,Bx+1) = (x+1&lt;W &amp;&amp; y&gt;0) ? in[(y-1)*W + (x+1)] : 0.f;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.x == 0 &amp;&amp; threadIdx.y == By-1) &#123;</span><br><span class="line">        t(By+1,0) = (x&gt;0 &amp;&amp; y+1&lt;H) ? in[(y+1)*W + (x-1)] : 0.f;</span><br><span class="line">    &#125;</span><br><span class="line">    if (threadIdx.x == Bx-1 &amp;&amp; threadIdx.y == By-1) &#123;</span><br><span class="line">        t(By+1,Bx+1) = (x+1&lt;W &amp;&amp; y+1&lt;H) ? in[(y+1)*W + (x+1)] : 0.f;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    if (x &lt; W &amp;&amp; y &lt; H) &#123;</span><br><span class="line">        float sum = 0.f;</span><br><span class="line">        sum += t(ty-1, tx-1); sum += t(ty-1, tx); sum += t(ty-1, tx+1);</span><br><span class="line">        sum += t(ty,   tx-1); sum += t(ty,   tx); sum += t(ty,   tx+1);</span><br><span class="line">        sum += t(ty+1, tx-1); sum += t(ty+1, tx); sum += t(ty+1, tx+1);</span><br><span class="line">        out[y * W + x] = sum / 9.f;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>共享内存大小（字节）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">((W + block.x - <span class="number">1</span>) / block.x, (H + block.y - <span class="number">1</span>) / block.y)</span></span>;</span><br><span class="line"><span class="type">size_t</span> shared_bytes = (block.x + <span class="number">2</span>) * (block.y + <span class="number">2</span>) * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">mean3x3&lt;&lt;&lt;grid, block, shared_bytes&gt;&gt;&gt;(in, out, H, W);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="10-CUDA-Streams-与异步拷贝"><a href="#10-CUDA-Streams-与异步拷贝" class="headerlink" title="10. CUDA Streams 与异步拷贝"></a>10. CUDA Streams 与异步拷贝</h2><h3 id="10-1-为什么需要-streams"><a href="#10-1-为什么需要-streams" class="headerlink" title="10.1 为什么需要 streams"></a>10.1 为什么需要 streams</h3><ul>
<li><p>默认 stream（stream 0）会形成较强的串行依赖</p>
</li>
<li><p>多 stream 可以实现：</p>
<ul>
<li>H2D 拷贝与 kernel 重叠</li>
<li>多批次流水线（pipeline）</li>
<li>与 <code>cudaMemcpyAsync</code> 配合提升吞吐</li>
</ul>
</li>
</ul>
<h3 id="10-2-pinned（页锁定）主机内存：提升异步拷贝效率"><a href="#10-2-pinned（页锁定）主机内存：提升异步拷贝效率" class="headerlink" title="10.2 pinned（页锁定）主机内存：提升异步拷贝效率"></a>10.2 pinned（页锁定）主机内存：提升异步拷贝效率</h3><ul>
<li><code>cudaMallocHost</code> &#x2F; <code>cudaFreeHost</code></li>
<li>pinned 内存更利于 DMA，<code>cudaMemcpyAsync</code> 才更有意义</li>
</ul>
<h3 id="10-3-基本模板：两条-stream-流水搬运"><a href="#10-3-基本模板：两条-stream-流水搬运" class="headerlink" title="10.3 基本模板：两条 stream 流水搬运"></a>10.3 基本模板：两条 stream 流水搬运</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;cstdio&gt;</span><br><span class="line">#include &lt;cuda_runtime.h&gt;</span><br><span class="line"></span><br><span class="line">#define CUDA_CHECK(call) do &#123;                                  \</span><br><span class="line">  cudaError_t err = (call);                                    \</span><br><span class="line">  if (err != cudaSuccess) &#123;                                    \</span><br><span class="line">    std::fprintf(stderr, &quot;CUDA error %s:%d: %s\n&quot;,             \</span><br><span class="line">      __FILE__, __LINE__, cudaGetErrorString(err));            \</span><br><span class="line">    std::exit(1);                                              \</span><br><span class="line">  &#125;                                                            \</span><br><span class="line">&#125; while(0)</span><br><span class="line"></span><br><span class="line">__global__ void scale(float* x, int n, float a) &#123;</span><br><span class="line">    for (int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        x[i] *= a;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    const int n = 1 &lt;&lt; 20;</span><br><span class="line">    const size_t bytes = n * sizeof(float);</span><br><span class="line"></span><br><span class="line">    // pinned host memory</span><br><span class="line">    float* h = nullptr;</span><br><span class="line">    CUDA_CHECK(cudaMallocHost(&amp;h, bytes));</span><br><span class="line"></span><br><span class="line">    // device memory</span><br><span class="line">    float* d = nullptr;</span><br><span class="line">    CUDA_CHECK(cudaMalloc(&amp;d, bytes));</span><br><span class="line"></span><br><span class="line">    // init host</span><br><span class="line">    for (int i = 0; i &lt; n; ++i) h[i] = 1.0f;</span><br><span class="line"></span><br><span class="line">    cudaStream_t s;</span><br><span class="line">    CUDA_CHECK(cudaStreamCreate(&amp;s));</span><br><span class="line"></span><br><span class="line">    // async H2D</span><br><span class="line">    CUDA_CHECK(cudaMemcpyAsync(d, h, bytes, cudaMemcpyHostToDevice, s));</span><br><span class="line"></span><br><span class="line">    // kernel in same stream (will wait for H2D in this stream)</span><br><span class="line">    scale&lt;&lt;&lt;256, 256, 0, s&gt;&gt;&gt;(d, n, 2.0f);</span><br><span class="line">    CUDA_CHECK(cudaGetLastError());</span><br><span class="line"></span><br><span class="line">    // async D2H</span><br><span class="line">    CUDA_CHECK(cudaMemcpyAsync(h, d, bytes, cudaMemcpyDeviceToHost, s));</span><br><span class="line"></span><br><span class="line">    // wait stream done</span><br><span class="line">    CUDA_CHECK(cudaStreamSynchronize(s));</span><br><span class="line"></span><br><span class="line">    std::printf(&quot;h[0]=%f, h[n-1]=%f\n&quot;, h[0], h[n-1]);</span><br><span class="line"></span><br><span class="line">    CUDA_CHECK(cudaStreamDestroy(s));</span><br><span class="line">    CUDA_CHECK(cudaFree(d));</span><br><span class="line">    CUDA_CHECK(cudaFreeHost(h));</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="10-4-事件计时（event-timing）"><a href="#10-4-事件计时（event-timing）" class="headerlink" title="10.4 事件计时（event timing）"></a>10.4 事件计时（event timing）</h3><p>用于测量 GPU 端耗时：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t st, ed;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;st); <span class="built_in">cudaEventCreate</span>(&amp;ed);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(st, stream);</span><br><span class="line"><span class="comment">// kernel / memcpyAsync ...</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(ed, stream);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(ed);</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> ms = <span class="number">0.f</span>;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;ms, st, ed);</span><br></pre></td></tr></table></figure>


            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://chutianduan.github.io/2026/01/04/%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%80%A7%E8%83%BDC++%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B/[08]CUDA%E5%BC%80%E5%90%AF%E7%9A%84GPU%E7%BC%96%E7%A8%8B/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://chutianduan.github.io/2026/01/04/%E5%AD%A6%E4%B9%A0/%E9%AB%98%E6%80%A7%E8%83%BDC++%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B/[08]CUDA%E5%BC%80%E5%90%AF%E7%9A%84GPU%E7%BC%96%E7%A8%8B/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
